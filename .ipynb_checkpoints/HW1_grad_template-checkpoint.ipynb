{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c05bd0",
   "metadata": {},
   "source": [
    "## *You need to finish code where \"...\" exists and add some Markdown cells to give answers based on the outputs when necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724ac4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "import time\n",
    "\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print(f'func:{f.__name__} took: {te-ts:.4f} sec')\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e797ae",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa527ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def func(X):\n",
    "    x, y = X\n",
    "    return ...\n",
    "\n",
    "def first_derivative(X):\n",
    "    ...\n",
    "\n",
    "def second_derivative(X):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b239b46",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043cb40",
   "metadata": {},
   "source": [
    "*For debugging*: new point should be $(0.15,0.9)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab448f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_point = ...\n",
    "new_point = ... \n",
    "\n",
    "if ...:\n",
    "    print(\"This is a good step\")\n",
    "else:\n",
    "    print(\"This is a bad step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f865d45",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab917cd3",
   "metadata": {},
   "source": [
    "*For debugging*: Take 41 steps to converge. Converge to point [-0.99999852,  0.99999607] with value -2.999999999985186.  took: 0.0020 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7360c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(func, first_derivative, starting_point, step_size, tol):\n",
    "    \"\"\"\n",
    "    Steepest Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func: Callable\n",
    "        Function to be minimized\n",
    "    first_derivative: Callable\n",
    "        First derivative of the function to be minimized\n",
    "    starting_point: np.ndarray\n",
    "        Starting point of minimization\n",
    "    step_size: float\n",
    "        Size of each gradient descent step\n",
    "    tol: float\n",
    "        If the norm of the gradient is smaller than tol, the minimization will terminate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res: dict\n",
    "        Optimization result\n",
    "    \"\"\"\n",
    "    deriv = first_derivative(starting_point)\n",
    "    count = 0\n",
    "    visited = []\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e6:\n",
    "        # calcualte new position\n",
    "        new_point = ...\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation smaller - it is a good step. what do you do?\n",
    "            ...\n",
    "        else:\n",
    "            # the step makes function evaluation larger - it is a bad step. what do you do?\n",
    "            ...\n",
    "        count += 1\n",
    "    return {\n",
    "        \"x\": starting_point,\n",
    "        \"evaluation\": func(starting_point),\n",
    "        \"path\": np.array(visited)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da369444",
   "metadata": {},
   "source": [
    "Funtion for drawing path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_path(func, path, x_min=-2, x_max=2, y_min=-2, y_max=2):\n",
    "    a = np.linspace(x_min, x_max, 100)\n",
    "    b = np.linspace(y_min, y_max, 100)\n",
    "    x, y = np.meshgrid(a, b)\n",
    "    z = func((x, y))\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = ax.contour(x, y, z, 50)\n",
    "    plt.colorbar(contour)\n",
    "    ax.plot(path[:, 0], path[:, 1], color='red')\n",
    "    print(\"Length of the path:\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b473763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization & draw path\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c038e48",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83934c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CG/BFGS optimization with scipy\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80464ce0",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(X):\n",
    "    return ...\n",
    "\n",
    "def Rosenbrock_grad(X):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa725c4",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_point = ...\n",
    "# SD minimization & draw path\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582a634",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(func, first_derivative, starting_point, step_size, tol, stochastic_injection=0):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func: Callable\n",
    "        Function to be minimized\n",
    "    first_derivative: Callable\n",
    "        First derivative of the function to be minimized\n",
    "    starting_point: np.ndarray\n",
    "        Starting point of minimization\n",
    "    step_size: float\n",
    "        Size of each gradient descent step\n",
    "    tol: float\n",
    "        If the norm of the gradient is smaller than tol, the minimization will terminate\n",
    "    stochastic_injection: int\n",
    "        Enable stochastic gradient (set to 1) or not (set to 0).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res: dict\n",
    "        Optimization result\n",
    "    \"\"\"\n",
    "    # evaluate the gradient of the starting point at first\n",
    "    deriv = ...\n",
    "    count = 0\n",
    "    visited = []\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e6:\n",
    "        if stochastic_injection > 0:\n",
    "            # formulate a stochastic_deriv (random vector) \n",
    "            # that is the same norm as your gradient \n",
    "            stochastic_deriv = ...\n",
    "        else:\n",
    "            stochastic_deriv = np.zeros(len(starting_point))\n",
    "        \n",
    "        direction = -(deriv + stochastic_injection * stochastic_deriv)\n",
    "        # new position\n",
    "        new_point = ...\n",
    "\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # good step\n",
    "            ...\n",
    "            step_size = ...\n",
    "        else:\n",
    "            # bad step\n",
    "            step_size = ...\n",
    "        count += 1\n",
    "    return {\n",
    "        \"x\": starting_point,\n",
    "        \"evaluation\": func(starting_point),\n",
    "        \"path\": np.array(visited)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f08cb7",
   "metadata": {},
   "source": [
    "*For debugging*: This is a stochastic method so your outputs may vary. For SGD, it takes ~1700 steps to converge and it takes ~0.1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_point = np.array([-0.5, 1.5])\n",
    "# SGD optimization and draw path\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94270794",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CG/BFGS with scipy\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35048ffc",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa9876",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_test(method, args, times, global_minimum=None):\n",
    "    result = []\n",
    "    for n in range(times):\n",
    "        res = method(*args)\n",
    "        \n",
    "        # If the norm of the differnce vector between the global minimum\n",
    "        # and the resulting point is larger than 1e-3, the point will not\n",
    "        # be considered as a global minimum\n",
    "        if (global_minimum is not None) and (...):\n",
    "            continue\n",
    "        result.append(...)\n",
    "    \n",
    "    msg = f\"Running {times} times: reach global minimum {len(result)} times\"\n",
    "    if len(result) > 0:\n",
    "        avg = ... # calculate the average\n",
    "        std = ... # calculate the standard deviation\n",
    "        msg += f\", average {int(avg)} steps with variance {std:.2f}\"\n",
    "    print(msg)\n",
    "\n",
    "starting_points = [\n",
    "    original_point,\n",
    "    np.array([0.0, 1.0]),\n",
    "    np.array([-1.0, 1.0]),\n",
    "    np.array([1.5, 0.5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different minimization algorithms for different starting points\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693960e",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69072d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Camel(X):\n",
    "    return ...\n",
    "\n",
    "def Camel_grad(X):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594a484",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504142c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_point = np.array([-1.5, -1.5])\n",
    "\n",
    "starting_points = [\n",
    "    original_point,\n",
    "    np.array([0.0, 1.0]),\n",
    "    np.array([-1.0, 1.0]),\n",
    "    np.array([1.5, 0.5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different minimization algorithms for different starting points\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1862d",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2224e8",
   "metadata": {},
   "source": [
    "*For debugging*: This is a stochastic method so your outputs may vary. For SGDM, it takes ~200 steps to converge and it takes ~0.02 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07461ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdm(func, first_derivative, starting_point, step_size, tol, stochastic_injection=0, momentum=0):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent with Momentum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func: Callable\n",
    "        Function to be minimized\n",
    "    first_derivative: Callable\n",
    "        First derivative of the function to be minimized\n",
    "    starting_point: np.ndarray\n",
    "        Starting point of minimization\n",
    "    step_size: float\n",
    "        Size of each gradient descent step\n",
    "    tol: float\n",
    "        If the norm of the gradient is smaller than tol, the minimization will terminate\n",
    "    stochastic_injection: int\n",
    "        Enable stochastic gradient (set to 1) or not (set to 0).\n",
    "    momentum: float\n",
    "        Momentum (\\eta) value in SGDM algorithm\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res: dict\n",
    "        Optimization result\n",
    "    \"\"\"\n",
    "    \n",
    "    deriv = first_derivative(starting_point)\n",
    "    count = 0\n",
    "    visited = []\n",
    "    previous_direction = np.zeros(len(starting_point))\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e6:\n",
    "        if stochastic_injection > 0:\n",
    "            # formulate a stochastic_deriv (random vector) \n",
    "            # that is the same norm as your gradient \n",
    "            stochastic_deriv = ...\n",
    "        else:\n",
    "            stochastic_deriv = np.zeros(len(starting_point))\n",
    "        \n",
    "        # new direction \n",
    "        direction = ...\n",
    "        # calculate new point, don't forget momentum!\n",
    "        new_point = ...\n",
    "\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # good step\n",
    "            ...\n",
    "        else:\n",
    "            # bad step\n",
    "            if step_size < 1e-5:\n",
    "                # step size too small, zero out the previous direction\n",
    "                # since we know it is a bad direction\n",
    "                previous_direction = np.zeros(len(starting_point))\n",
    "            else:\n",
    "                step_size = ...\n",
    "        count += 1\n",
    "    \n",
    "    return {\n",
    "        \"x\": starting_point,\n",
    "        \"evaluation\": func(starting_point),\n",
    "        \"path\": np.array(visited)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SGDM\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c142",
   "language": "python",
   "name": "c142"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
